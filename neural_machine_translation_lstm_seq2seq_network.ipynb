{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerlaxrex/keras-seq2seq-models/blob/main/neural_machine_translation_lstm_seq2seq_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NMT with Tensorflow and Keras\n",
        "\n",
        "In this notebook we build a simple Encoder-Decoder Network with Attention mechanism using Tensorflow and Keras APIs.\n",
        "The first network is built using only vanilla Tensorflow/Keras by creating three modules: Encoder, LuongAttention and Decoder.\n",
        "\n",
        "After training and evaluating some sentences I use the Tensorflow Addons (tfa) library in order to train the model. In this case, the training step is customized in order to allow a better control over the various steps of the decoding process.\n",
        "\n",
        "The decoding phase is greedy, but I plan to try also a Beam Search decoding.\n",
        "\n",
        "The dataset is composed of english sentences and the italian translation of them. In this case, we try to translate from english to italian."
      ],
      "metadata": {
        "id": "o4zPh3842nj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aws84QfqDDm",
        "outputId": "8e10e77f-6fd4-4d83-ebce-0bddf1769c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datatable in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: afinn in /usr/local/lib/python3.7/dist-packages (0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "! pip install datatable\n",
        "! pip install afinn\n",
        "! pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HRr6GbS1IHAh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datatable as dt\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from afinn import Afinn\n",
        "\n",
        "import sys, gc, re, time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, GRU, Attention, concatenate, LSTMCell\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "z6AJ6JMyIHAr"
      },
      "outputs": [],
      "source": [
        "NUM_DATA = 300_000\n",
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T14kC7beGwQt"
      },
      "source": [
        "**WARNING**: Before doing this step you need to go to Kaggle and generate the API token for your personal account. Then upload here on Colab the \"kaggle.json\" file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Uv0uI0cqR_e",
        "outputId": "47e3e034-2b42-45ff-a81e-4ba18bb83b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.5.18.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "nmtitalian2english.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  nmtitalian2english.zip\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: _about.txt              \n",
            "  inflating: ita.txt                 \n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! \\cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d balamurugan1603/nmtitalian2english\n",
        "# ! kaggle datasets download -d yelp-dataset/yelp-dataset\n",
        "! unzip nmtitalian2english.zip\n",
        "# ! unzip yelp-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ua0SpP6-aIz"
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "We perform a small preprocessing of the data consisting in:\n",
        "\n",
        "\n",
        "\n",
        "1.   Shuffling the data once\n",
        "2.   Insert a whitespace between a word and puntuation (?, !, ...)\n",
        "3.   Reverse the source sentences (this is done in order to make the beginning of the sentence more \"relevant\" in the encoding phase)\n",
        "4.   Add the special tokens for decoding in the form of \"\\<SOS\\>\" and \"\\<EOS\\>\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Z37z6coeIHAt"
      },
      "outputs": [],
      "source": [
        "dataset = dt.fread('../content/ita.txt',fill=True,header=False,columns=['en','it','info']).to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4EUju4JIIHAw"
      },
      "outputs": [],
      "source": [
        "def shuffle(df):\n",
        "    np.random.shuffle(df.values)\n",
        "\n",
        "def preprocess_sc(df):\n",
        "    df['src'] = df['en'].map(lambda x : re.sub( r'([!?.,;])', r' \\1', x)).map(lambda x : ' '.join(x.split()[::-1]))\n",
        "    df['tgt'] = df['it'].map(lambda x : re.sub( r'([!?.,;])', r' \\1', x))\n",
        "\n",
        "def delete_na(df):\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "def add_special_symbols(df):\n",
        "    df['tgt'] = df['tgt'].map(lambda x : '<SOS> ' + x + ' <EOS>') \n",
        "\n",
        "def preprocessing_text(df):\n",
        "    shuffle(df)\n",
        "    delete_na(df)\n",
        "    preprocess_sc(df)\n",
        "    add_special_symbols(df)\n",
        "    df.drop(columns=['info','it','en'], inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Fq-OxZKdIHAy"
      },
      "outputs": [],
      "source": [
        "data = preprocessing_text(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e03ABU_sIHA0",
        "outputId": "2dffa591-c12b-40d0-e033-d44b283773b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "src                       . child a as Japan to came Tom\n",
              "tgt    <SOS> Tom è venuto in Giappone da bambino . <EOS>\n",
              "Name: 100, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "data.iloc[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JPKHt0A8L8J"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Uh_s41ewIHA1"
      },
      "outputs": [],
      "source": [
        "tokenizer_src = Tokenizer(num_words=20_000, filters='\"#$%&()*+-/=@[\\\\]^_`{|}~\\t\\n', oov_token='<oov>')\n",
        "tokenizer_src.fit_on_texts(data['src'])\n",
        "\n",
        "tokenizer_tgt = Tokenizer(num_words=20_000, filters='\"#$%&()*+-/=@[\\\\]^_`{|}~\\t\\n', oov_token='<oov>')\n",
        "tokenizer_tgt.fit_on_texts(data['tgt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RGCn5kwrIHA4"
      },
      "outputs": [],
      "source": [
        "src_sequences = tokenizer_src.texts_to_sequences(data['src'])\n",
        "tgt_sequences = tokenizer_tgt.texts_to_sequences(data['tgt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3qc82ymIHA6",
        "outputId": "a4122caa-7054-4b0c-cc22-a9cc18af5331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "111 103\n"
          ]
        }
      ],
      "source": [
        "max_src_len = max(len(s) for s in src_sequences)\n",
        "max_tgt_len = max(len(s) for s in tgt_sequences)\n",
        "print(max_src_len, max_tgt_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMOGOokhIHA9",
        "outputId": "8cdbbf79-2cd2-4df1-c273-9c4ec825007c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total data: 352_035, \n",
            "training data: 225_302, \n",
            "Validation data: 56_325, \n",
            "Test data: 70_407\n"
          ]
        }
      ],
      "source": [
        "tot_length = len(src_sequences)\n",
        "data_length = int(tot_length*0.8)\n",
        "train_length = int(data_length*0.8)\n",
        "valid_length = int(data_length*0.2)\n",
        "test_length = int(tot_length*0.2)\n",
        "print(f'Total data: {tot_length:_}, \\ntraining data: {train_length:_}, \\nValidation data: {valid_length:_}, \\nTest data: {test_length:_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Tc4vUWtdIHA-"
      },
      "outputs": [],
      "source": [
        "#Split the training, validation and test sets\n",
        "train_idx_end, valid_idx_start = train_length, train_length\n",
        "valid_idx_end = train_length+valid_length\n",
        "test_idx_start = train_length+valid_length\n",
        "\n",
        "train_sequences = [src_sequences[:train_idx_end], tgt_sequences[:train_idx_end]]\n",
        "valid_sequences = [src_sequences[valid_idx_start:valid_idx_end], tgt_sequences[valid_idx_start:valid_idx_end]]\n",
        "test_sequences = [src_sequences[test_idx_start:], tgt_sequences[test_idx_start:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE839DUBIHBA",
        "outputId": "928de771-ad37-4902-b722-96919b329d2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('. truth the telling in hesitation no have i',\n",
              " '<sos> non ho alcuna esitazione a dire la verità . <eos>')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Try the preprocessing\n",
        "src_str = ' '.join(tokenizer_src.index_word[i] for i in train_sequences[0][0])\n",
        "tgt_str = ' '.join(tokenizer_tgt.index_word[i] for i in train_sequences[1][0])\n",
        "src_str, tgt_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZQXcy61CgiM"
      },
      "source": [
        "## Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "FEy6Ax_WiJAQ"
      },
      "outputs": [],
      "source": [
        "BS_TRAIN = 64\n",
        "BS_VALID = 32\n",
        "SPE_TRAIN = len(train_sequences[0])//BS_TRAIN\n",
        "SPE_VALID = len(valid_sequences[0])//BS_VALID\n",
        "EPOCHS = 10\n",
        "\n",
        "def create_dataset(src_seqs, tgt_seqs, bs_size, maxlen=20, padding='post'):\n",
        "  tgt_1 = [w[:-1] for w in tgt_seqs]\n",
        "  tgt_2 = [w[1:] for w in tgt_seqs]\n",
        "\n",
        "  src_ds = tf.data.Dataset.from_tensor_slices((pad_sequences(src_seqs, maxlen=20, padding='post'), \n",
        "                                              pad_sequences(tgt_1, maxlen=20, padding='post')))\n",
        "  tgt_ds = tf.data.Dataset.from_tensor_slices(pad_sequences(tgt_2, maxlen=20, padding='post'))\n",
        "\n",
        "  gen_ds = tf.data.Dataset.zip((src_ds, tgt_ds))\n",
        "  gen_ds = gen_ds.repeat().shuffle(1000).batch(bs_size).prefetch(1)\n",
        "  return gen_ds\n",
        "\n",
        "gen_train_ds = create_dataset(train_sequences[0], train_sequences[1], bs_size=BS_TRAIN)\n",
        "gen_valid_ds = create_dataset(valid_sequences[0], valid_sequences[1], bs_size=BS_VALID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4txGQWgX8YbB"
      },
      "source": [
        "## Encoder/Decoder Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "kbfYVtTcIHBG"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, num_words, embedding_size=100, lstm_out_dim = 128, name='Encoder_model'):\n",
        "        super(Encoder,self).__init__(name=name)\n",
        "        self.lstm_output_dim = lstm_out_dim\n",
        "        self.embedding_size = embedding_size\n",
        "        self.embedding_layer = Embedding(input_dim=num_words+1,\n",
        "                                         output_dim=self.embedding_size,\n",
        "                                         mask_zero = True,\n",
        "                                         name='Encoder_embedding')\n",
        "        \n",
        "        self.lstm_layer = LSTM(units = self.lstm_output_dim, \n",
        "                                return_sequences = True,\n",
        "                                return_state = True, \n",
        "                                name='Encoder_LSTM')\n",
        "    \n",
        "    def call(self, inputs, init_state = None):\n",
        "        #Return the whole sequence and the last state\n",
        "        x = self.embedding_layer(inputs)\n",
        "        mask = self.embedding_layer.compute_mask(inputs)\n",
        "        outs, h, c = self.lstm_layer(x, mask = mask, initial_state = init_state)\n",
        "        return outs, h, c\n",
        "\n",
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, latent_dim, name='Luong_Attention'):\n",
        "    super().__init__()\n",
        "    self.W1 = Dense(units=latent_dim)\n",
        "    self.W2 = Dense(units=latent_dim)\n",
        "    self.attn = Attention()\n",
        "\n",
        "  def call(self, query, value):\n",
        "    enc_states = self.W1(value)\n",
        "    dec_last_state = self.W2(query)\n",
        "    # print(enc_states.shape)\n",
        "    # print(dec_last_state.shape)\n",
        "    attn_out, attn_scores = self.attn([dec_last_state, enc_states], return_attention_scores=True)\n",
        "    return attn_out, attn_scores\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_words, embedding_size = 100, lstm_out_dim = 128, name = 'Decoder_model'):\n",
        "        super(Decoder,self).__init__(name=name)\n",
        "        self.lstm_output_dim = lstm_out_dim\n",
        "        self.embedding_size = embedding_size\n",
        "        self.embedding_layer = Embedding(input_dim=num_words+1,\n",
        "                                         output_dim=self.embedding_size,\n",
        "                                         mask_zero=True,\n",
        "                                         name = 'Decoder_embedding')\n",
        "        \n",
        "        self.lstm_layer = LSTM(units = self.lstm_output_dim, \n",
        "                               return_state = True, \n",
        "                               return_sequences = True, \n",
        "                               name='Decoder_LSTM')\n",
        "        \n",
        "        self.attention_layer = LuongAttention(embedding_size)\n",
        "        self.dense_decode_layer = TimeDistributed(Dense(units = lstm_out_dim, activation='tanh'))\n",
        "\n",
        "        self.final_layer = TimeDistributed(Dense(units = num_words))\n",
        "    def call(self, inputs, encoder_states):\n",
        "        # With teacher forcing we give in input the expected tgt[:-1] and compare the result with the tgt[1:]\n",
        "        # Without teacher forcing we use the previous decoder output and state for the next step \n",
        "        # (this is why we return results and outputs/states).\n",
        "        x = self.embedding_layer(inputs)\n",
        "        mask = self.embedding_layer.compute_mask(inputs)\n",
        "        out, h, c = self.lstm_layer(x, initial_state = encoder_states[1:], mask = mask)\n",
        "\n",
        "        #Adding attention mechanism\n",
        "        context_vector, attn_weights = self.attention_layer(out, encoder_states[0])\n",
        "        concat_out = concatenate([context_vector, out])\n",
        "        out = self.dense_decode_layer(concat_out)\n",
        "\n",
        "        #Final results\n",
        "        res = self.final_layer(out)   \n",
        "        del x,out\n",
        "        gc.collect()\n",
        "        return res, h, c, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try encoder Shapes"
      ],
      "metadata": {
        "id": "nzrvTwAr8B9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPf1wK8lY_Qw",
        "outputId": "d1efdfd3-1bcb-4ce6-fbfb-2d5372674df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 9, 128) (1, 128) (1, 128)\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(100)\n",
        "a,b,c = encoder(tf.expand_dims(train_sequences[0][0], 0))\n",
        "print(a.shape, b.shape, c.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try LuongAttention shapes and viz"
      ],
      "metadata": {
        "id": "xqLwsWxs8E3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "rz0cnU1Ceyew",
        "outputId": "5ff265d3-ffe6-4f79-bf05-84454a6ba53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context vector shape: (1, 12, 100) - attention weights shape: (1, 12, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f97e19e3510>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "luong_attn = LuongAttention(latent_dim = 100)\n",
        "value = np.random.rand(1,10,100) #Encoder states\n",
        "query = np.random.rand(1,12,100) #Decoder outputs\n",
        "masks = [np.ones((1,12), dtype=bool), np.ones((1,10), dtype=bool)]\n",
        "context_vector, attn_weights = luong_attn(query, value)\n",
        "print(f'Context vector shape: {context_vector.shape} - attention weights shape: {attn_weights.shape}')\n",
        "\n",
        "sns.heatmap(tf.squeeze(attn_weights).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building\n",
        "\n",
        "Build the Encoder/Decoder Graph"
      ],
      "metadata": {
        "id": "Hf-xHZDN8InL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ErtyhmIHBH",
        "outputId": "e5be6aba-ae85-4af4-95c5-0978995b02fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 128) (None, 128) (None, 128)\n"
          ]
        }
      ],
      "source": [
        "encoder_input = tf.keras.Input(shape=[None], name='Encoder_inputs')\n",
        "decoder_input = tf.keras.Input(shape=[None], name='Decoder_inputs')\n",
        "\n",
        "encoder = Encoder(num_words = len(tokenizer_src.word_index), embedding_size=50, lstm_out_dim=128)\n",
        "decoder = Decoder(num_words = len(tokenizer_tgt.word_index), embedding_size=50, lstm_out_dim=128)\n",
        "\n",
        "#Build the graph\n",
        "outs, h_tot, c_tot = encoder(encoder_input)\n",
        "print(outs.shape, h_tot.shape, c_tot.shape)\n",
        "\n",
        "\n",
        "return_seq,_,_,_ = decoder(decoder_input, encoder_states=[outs, h_tot, c_tot])\n",
        "\n",
        "seq2seq_model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=return_seq, name='Seq2Seq_Model')\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=0.5e-3)\n",
        "seq2seq_model.compile(loss=loss, optimizer=optim, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW-k33CvIHBI",
        "outputId": "cd884155-553b-42a1-b4ba-5c4e111e4323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Seq2Seq_Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Encoder_model (Encoder)        ((None, None, 128),  787898      ['Encoder_inputs[0][0]']         \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128))                                                     \n",
            "                                                                                                  \n",
            " Decoder_model (Decoder)        ((None, None, 28497  5228473     ['Decoder_inputs[0][0]',         \n",
            "                                ),                                'Encoder_model[0][0]',          \n",
            "                                 (None, 128),                     'Encoder_model[0][1]',          \n",
            "                                 (None, 128),                     'Encoder_model[0][2]']          \n",
            "                                 (None, None, None)                                               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,016,371\n",
            "Trainable params: 6,016,371\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"Encoder_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Encoder_embedding (Embeddin  multiple                 696250    \n",
            " g)                                                              \n",
            "                                                                 \n",
            " Encoder_LSTM (LSTM)         multiple                  91648     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 787,898\n",
            "Trainable params: 787,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"Decoder_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Decoder_embedding (Embeddin  multiple                 1424900   \n",
            " g)                                                              \n",
            "                                                                 \n",
            " Decoder_LSTM (LSTM)         multiple                  91648     \n",
            "                                                                 \n",
            " luong_attention_5 (LuongAtt  multiple                 12900     \n",
            " ention)                                                         \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDis  multiple                 22912     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " time_distributed_7 (TimeDis  multiple                 3676113   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,228,473\n",
            "Trainable params: 5,228,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "seq2seq_model.summary()\n",
        "encoder.summary()\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "PgHIJavJ5Gg9"
      },
      "outputs": [],
      "source": [
        "# Define the callbacks for our model\n",
        "\n",
        "callbacksList = [\n",
        "                 tf.keras.callbacks.ModelCheckpoint('./chkpnt/model_saving', save_best_only=True, save_weights_only=True),\n",
        "                 tf.keras.callbacks.EarlyStopping(patience=3)\n",
        "                #  tf.keras.callbacks.TensorBoard()\n",
        "                 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKBJXFEpIHBM",
        "outputId": "8d25c728-b9c8-41db-9c62-2a65552b5365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3520/3520 [==============================] - 374s 103ms/step - loss: 1.6475 - accuracy: 0.3260 - val_loss: 1.2493 - val_accuracy: 0.4457\n",
            "Epoch 2/10\n",
            "3520/3520 [==============================] - 359s 102ms/step - loss: 0.9967 - accuracy: 0.5365 - val_loss: 0.8059 - val_accuracy: 0.6108\n",
            "Epoch 3/10\n",
            "3520/3520 [==============================] - 359s 102ms/step - loss: 0.6899 - accuracy: 0.6535 - val_loss: 0.6328 - val_accuracy: 0.6764\n",
            "Epoch 4/10\n",
            "3520/3520 [==============================] - 359s 102ms/step - loss: 0.5467 - accuracy: 0.7067 - val_loss: 0.5374 - val_accuracy: 0.7124\n",
            "Epoch 5/10\n",
            "3520/3520 [==============================] - 359s 102ms/step - loss: 0.4643 - accuracy: 0.7364 - val_loss: 0.4833 - val_accuracy: 0.7323\n",
            "Epoch 6/10\n",
            "3520/3520 [==============================] - 358s 102ms/step - loss: 0.4093 - accuracy: 0.7559 - val_loss: 0.4451 - val_accuracy: 0.7460\n",
            "Epoch 7/10\n",
            "3520/3520 [==============================] - 358s 102ms/step - loss: 0.3691 - accuracy: 0.7709 - val_loss: 0.4171 - val_accuracy: 0.7560\n",
            "Epoch 8/10\n",
            "3520/3520 [==============================] - 358s 102ms/step - loss: 0.3384 - accuracy: 0.7826 - val_loss: 0.3963 - val_accuracy: 0.7642\n",
            "Epoch 9/10\n",
            "3520/3520 [==============================] - 360s 102ms/step - loss: 0.3138 - accuracy: 0.7921 - val_loss: 0.3807 - val_accuracy: 0.7699\n",
            "Epoch 10/10\n",
            "3520/3520 [==============================] - 360s 102ms/step - loss: 0.2934 - accuracy: 0.8006 - val_loss: 0.3647 - val_accuracy: 0.7764\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f988638af10>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "seq2seq_model.fit(gen_train_ds, \n",
        "                  epochs=EPOCHS, \n",
        "                  steps_per_epoch=SPE_TRAIN,\n",
        "                  validation_data=gen_valid_ds,\n",
        "                  validation_steps=SPE_VALID\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "i2MCpkpNCc0R"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "MFGmLFYAVJJD"
      },
      "outputs": [],
      "source": [
        "# %tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "y61ldQQLIHBO"
      },
      "outputs": [],
      "source": [
        "def evaluate_sentence(sentence, debug=False, viz_attn=False):\n",
        "  sentence = [re.sub(r'([!?.,;])', r' \\1', sentence)]\n",
        "  sentence = [' '.join(sentence[0].split()[::-1])]\n",
        "  seq_sentence = tokenizer_src.texts_to_sequences(sentence)\n",
        "  seq_sentence = tf.convert_to_tensor(seq_sentence)\n",
        "  #Pass the sequence to the encoder\n",
        "  enc_out, enc_h, enc_c = encoder(seq_sentence)\n",
        "  dec_h = enc_h\n",
        "  dec_c = enc_c\n",
        "\n",
        "  out_seq = []\n",
        "  eos_idx = tokenizer_tgt.word_index['<eos>']\n",
        "  curr_output = [tokenizer_tgt.word_index['<sos>']]\n",
        "  for i in range(20):\n",
        "      dec_input = tf.convert_to_tensor(curr_output)\n",
        "      dec_input = tf.expand_dims(dec_input, axis=0)\n",
        "      if debug:\n",
        "        print(f'Input: {dec_input}')\n",
        "      out, h_, c_, _= decoder(dec_input, encoder_states=[enc_out, dec_h, dec_c])\n",
        "      out_idx = tf.squeeze(tf.argmax(out, axis=-1))\n",
        "      if debug:\n",
        "        print(f'Next output: {out_idx}')\n",
        "      dec_h = h_\n",
        "      dec_c = c_\n",
        "      \n",
        "      if out_idx == eos_idx:\n",
        "          break\n",
        "      \n",
        "      out_seq.append(out_idx.numpy())\n",
        "      if debug:\n",
        "        print(f'Output sequence: {out_seq}')\n",
        "      curr_output = [out_idx.numpy()]\n",
        "\n",
        "  print('Final sequence: '+' '.join(tokenizer_tgt.index_word[i] for i in out_seq))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm40hjklm4U2",
        "outputId": "003fbb6b-3f92-41e5-95db-5fe625bc5ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final sequence: io voglio uccidervi .\n",
            "Final sequence: cosa fate ?\n",
            "Final sequence: questa è una barzelletta ?\n",
            "Final sequence: non il mio sapore .\n",
            "Final sequence: io sono annoiato .\n",
            "Final sequence: c'è qualcosa che ti sta disturbando ?\n"
          ]
        }
      ],
      "source": [
        "evaluate_sentence(\"i want to kill you\")\n",
        "evaluate_sentence(\"What are you doing?\")\n",
        "evaluate_sentence(\"Is this a joke?\")\n",
        "evaluate_sentence(\"Not my problem\")\n",
        "evaluate_sentence(\"I'm bored\")\n",
        "evaluate_sentence(\"Is there something that is bothering you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWYsp5MHDAN-"
      },
      "source": [
        "# Using TF-Addons: Seq2Seq module\n",
        "\n",
        "The Encoder is already done. We need to define another type of Decoder that uses the Attention mechanism and the training sampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzXNUVIODG52"
      },
      "outputs": [],
      "source": [
        "class DecoderAddon(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_words, embedding_size = 100, lstm_out_dim = 128, name = 'Decoder_model'):\n",
        "    super().__init__()\n",
        "    self.num_words = num_words\n",
        "    self.embedding_size = embedding_size\n",
        "    self.lstm_out_dim = lstm_out_dim\n",
        "    self.name\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "\n",
        "    self.embedding_layer = Embedding(input_dim=num_words+1,\n",
        "                                         output_dim=self.embedding_size,\n",
        "                                         name = 'Decoder_embedding')\n",
        "    \n",
        "    self.rnn_cell = LSTMCell(units=lstm_out_dim,\n",
        "                             name='Decoder_LSTM_Cell')\n",
        "    \n",
        "    self.attn_mechanism = tfa.seq2seq.LuongAttention(units=lstm_out_dim)\n",
        "    self.recurrent_layer = tfa.seq2seq.AttentionWrapper(self.rnn_cell, \n",
        "                                                        self.attn_mechanism, \n",
        "                                                        attention_layer_size=self.lstm_out_dim)\n",
        "    self.prediction_layer = Dense(units=self.num_words)\n",
        "    self.decoder_layer = tfa.seq2seq.BasicDecoder(self.recurrent_layer, sampler=self.sampler, output_layer=self.prediction_layer)\n",
        "\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.recurrent_layer.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state  \n",
        "\n",
        "  def call(self, inputs, encoder_states):\n",
        "    x = self.embedding_layer(inputs)\n",
        "    mask = self.embedding_layer.compute_mask(inputs)\n",
        "    outputs, _, _ = self.decoder_layer(x, initial_state=encoder_states, mask = mask)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p0RKV1RMVAo"
      },
      "outputs": [],
      "source": [
        "encoder_addon = Encoder(len(tokenizer_src.word_index), embedding_size=50, lstm_out_dim=128)\n",
        "decoder_addon = DecoderAddon(len(tokenizer_tgt.word_index), embedding_size=50, lstm_out_dim=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "nneBIE7NOJA_"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric_fn = tf.keras.metrics.SparseCategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "iGmUm89vMhHN"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(enc_inp, dec_inp, dec_out, enc_hidden=None):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_h, enc_c = encoder_addon(enc_inp, enc_hidden)\n",
        "\n",
        "    # Set the AttentionMechanism object with encoder_outputs\n",
        "    decoder_addon.attn_mechanism.setup_memory(enc_output)\n",
        "\n",
        "    # Create AttentionWrapperState as initial_state for decoder\n",
        "    decoder_initial_state = decoder_addon.build_initial_state(BS_TRAIN, [enc_h, enc_c], tf.float32)\n",
        "    pred = decoder_addon(dec_inp, decoder_initial_state)\n",
        "    logits = pred.rnn_output\n",
        "    loss = loss_fn(dec_out, logits)\n",
        "    reshaped_logits = tf.reshape(tf.nn.softmax(logits), (logits.shape[0],dec_out.shape[-1], logits.shape[-1]))\n",
        "    #Update metric state\n",
        "    metric_fn.update_state(dec_out, reshaped_logits)\n",
        "\n",
        "  variables = encoder_addon.trainable_variables + decoder_addon.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss\n",
        "\n",
        "@tf.function\n",
        "def valid_step(enc_inp, dec_inp, dec_out, enc_hidden=None):\n",
        "  loss = 0\n",
        "\n",
        "  enc_output, enc_h, enc_c = encoder_addon(enc_inp, enc_hidden)\n",
        "  decoder_addon.attn_mechanism.setup_memory(enc_output)\n",
        "  # Create AttentionWrapperState as initial_state for decoder\n",
        "  decoder_initial_state = decoder_addon.build_initial_state(BS_VALID, [enc_h, enc_c], tf.float32)\n",
        "  pred = decoder_addon(dec_inp, decoder_initial_state)\n",
        "  logits = pred.rnn_output\n",
        "  loss = loss_fn(dec_out, logits)\n",
        "  reshaped_logits = tf.reshape(tf.nn.softmax(logits), (logits.shape[0],dec_out.shape[-1], logits.shape[-1]))\n",
        "  #Update metric state\n",
        "  metric_fn.update_state(dec_out, reshaped_logits)\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xUbj0y_Oia2",
        "outputId": "90a8a1d1-2eb1-491d-ee6b-1781e507a420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started training.\n",
            "Train Epoch 1 Batch 1800 Loss 0.456 Accuracy 0.881"
          ]
        }
      ],
      "source": [
        "print('Started training.')\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = None\n",
        "  total_loss = 0\n",
        "  total_acc = 0\n",
        "  valid_loss = 0\n",
        "  valid_acc = 0\n",
        "\n",
        "  #Train on dataset\n",
        "  metric_fn.reset_state()\n",
        "  for (batch, ((enc_inp, dec_inp), dec_out)) in enumerate(gen_train_ds.take(SPE_TRAIN)):\n",
        "    batch_loss = train_step(enc_inp, dec_inp, dec_out, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    total_acc = metric_fn.result()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('\\r '*200,end='')\n",
        "      print('\\rTrain Epoch {} Batch {} Loss {:.3f} Accuracy {:.3f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy(),\n",
        "                                                   total_acc),end='')\n",
        "  metric_fn.reset_state()\n",
        "  print('-----')\n",
        "  for (batch, ((enc_inp, dec_inp), dec_out)) in enumerate(gen_valid_ds.take(SPE_VALID)):\n",
        "    batch_valid_loss = valid_step(enc_inp, dec_inp, dec_out, enc_hidden)\n",
        "    valid_loss += batch_valid_loss\n",
        "    valid_acc = metric_fn.result()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('\\r '*200)\n",
        "      print('\\rValid Epoch {} Batch {} Loss {:.4f} Accuracy {:.3f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   valid_loss.numpy(),\n",
        "                                                   valid_acc), end='')\n",
        "    \n",
        "  print(f'Epoch {epoch+1} - ({time.time()-start}s) - Loss {total_loss/SPE_TRAIN:.3f} - Acc. {total_acc:.3f} - Val.Loss {valid_loss/SPE_VALID:.3f} - Val.Acc {valid_acc:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_sentence(sentence_in, debug=False, viz_attn=False):\n",
        "  sentence = [re.sub(r'([!?.,;])', r' \\1', sentence_in)]\n",
        "  sentence = [' '.join(sentence[0].split()[::-1])]\n",
        "  seq_sentence = tokenizer_src.texts_to_sequences(sentence)\n",
        "  seq_sentence = tf.convert_to_tensor(seq_sentence)\n",
        "  #Pass the sequence to the encoder\n",
        "\n",
        "  enc_start_state = [tf.zeros((1, encoder_addon.lstm_output_dim)), tf.zeros((1, encoder_addon.lstm_output_dim))]\n",
        "  enc_out, enc_h, enc_c = encoder_addon(seq_sentence)\n",
        "\n",
        "  out_seq = []\n",
        "  eos_idx = tokenizer_tgt.word_index['<eos>']\n",
        "  curr_output = [tokenizer_tgt.word_index['<sos>']]\n",
        "\n",
        "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder_addon.recurrent_layer, sampler=greedy_sampler, output_layer=decoder_addon.prediction_layer)\n",
        "  decoder_addon.attn_mechanism.setup_memory(enc_out)\n",
        "  # set decoder_initial_state\n",
        "  decoder_initial_state = decoder_addon.build_initial_state(1, [enc_h, enc_c], tf.float32)\n",
        "\n",
        "\n",
        "  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
        "  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
        "\n",
        "  decoder_embedding_matrix = decoder_addon.embedding_layer.variables[0]\n",
        "\n",
        "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = curr_output, end_token= eos_idx, initial_state=decoder_initial_state)\n",
        "  out_seq = outputs.sample_id.numpy().squeeze()\n",
        "  print(f'Original sentence: {sentence_in}')\n",
        "  print('Final sequence: '+' '.join(tokenizer_tgt.index_word[i] for i in out_seq[:-1]))"
      ],
      "metadata": {
        "id": "buNqs4ikiz6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_sentence('I need to talk!')"
      ],
      "metadata": {
        "id": "s_rYkcYgp6QI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "neural_machine_translation_lstm_seq2seq_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}